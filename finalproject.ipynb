{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ+2kuagaPqyqWZCJSeqkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iirusia/Reinforcement-learning/blob/main/finalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw9SS18EYzn1",
        "outputId": "725d531b-8b58-43d1-e78c-bfe81b9460f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1: Reward = 342419523.32117474, Average Reward = 3424195.2332117474\n",
            "Predicted price for products after 100 episodes: 891원\n",
            "Model 2: Reward = 301014868.4667084, Average Reward = 3010148.6846670844\n",
            "Predicted price for products after 200 episodes: 759원\n",
            "Model 3: Reward = 243066889.54307655, Average Reward = 2430668.8954307656\n",
            "Predicted price for products after 300 episodes: 589원\n",
            "Model 4: Reward = 191661650.07839215, Average Reward = 1916616.5007839215\n",
            "Predicted price for products after 400 episodes: 515원\n",
            "Model 5: Reward = 285389086.21666557, Average Reward = 2853890.8621666557\n",
            "Predicted price for products after 500 episodes: 461원\n",
            "Model 6: Reward = 250852140.15206975, Average Reward = 2508521.4015206974\n",
            "Predicted price for products after 600 episodes: 566원\n",
            "Model 7: Reward = 357649197.36136913, Average Reward = 3576491.9736136915\n",
            "Predicted price for products after 700 episodes: 923원\n",
            "Model 8: Reward = 298371321.1289268, Average Reward = 2983713.211289268\n",
            "Predicted price for products after 800 episodes: 703원\n",
            "Model 9: Reward = 182784279.13374156, Average Reward = 1827842.7913374156\n",
            "Predicted price for products after 900 episodes: 468원\n",
            "Model 10: Reward = 425695257.5701956, Average Reward = 4256952.575701956\n",
            "Predicted price for products after 1000 episodes: 954원\n",
            "Performance metrics per 100 episodes:\n",
            "100 episodes from 1 to 100: Average Reward = 3424195.2332117474\n",
            "100 episodes from 101 to 200: Average Reward = 3010148.6846670844\n",
            "100 episodes from 201 to 300: Average Reward = 2430668.8954307656\n",
            "100 episodes from 301 to 400: Average Reward = 1916616.5007839215\n",
            "100 episodes from 401 to 500: Average Reward = 2853890.8621666557\n",
            "100 episodes from 501 to 600: Average Reward = 2508521.4015206974\n",
            "100 episodes from 601 to 700: Average Reward = 3576491.9736136915\n",
            "100 episodes from 701 to 800: Average Reward = 2983713.211289268\n",
            "100 episodes from 801 to 900: Average Reward = 1827842.7913374156\n",
            "100 episodes from 901 to 1000: Average Reward = 4256952.575701956\n",
            "Predicted prices per 100 episodes:\n",
            "After 100 episodes: Predicted price = 891원\n",
            "After 200 episodes: Predicted price = 759원\n",
            "After 300 episodes: Predicted price = 589원\n",
            "After 400 episodes: Predicted price = 515원\n",
            "After 500 episodes: Predicted price = 461원\n",
            "After 600 episodes: Predicted price = 566원\n",
            "After 700 episodes: Predicted price = 923원\n",
            "After 800 episodes: Predicted price = 703원\n",
            "After 900 episodes: Predicted price = 468원\n",
            "After 1000 episodes: Predicted price = 954원\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "\n",
        "class PricingEnv:\n",
        "    def __init__(self):\n",
        "        self.min_price = 400\n",
        "        self.max_price = 1000\n",
        "        self.nearby_price = 1000\n",
        "        self.action_space = np.arange(self.min_price, self.max_price + 1)\n",
        "        self.state = self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = [np.random.uniform(8, 15), np.random.choice([0, 1]), np.random.randint(100, 10000)]\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        min_wage, weather, population = self.state\n",
        "        new_price = action\n",
        "\n",
        "        if weather == 1:  # 흐림\n",
        "            adjusted_population = population * 0.6  # 40% 감소\n",
        "        else:\n",
        "            adjusted_population = population\n",
        "\n",
        "        sales = new_price * adjusted_population * np.random.uniform(0.5, 1.5)\n",
        "        cost = min_wage * population * 0.1\n",
        "        reward = sales - cost\n",
        "\n",
        "        if new_price < self.nearby_price * 0.5:\n",
        "            reward -= (self.nearby_price * 0.5 - new_price) * population\n",
        "\n",
        "        new_min_wage = min_wage + np.random.uniform(-0.5, 0.5)\n",
        "        new_population = population + np.random.randint(-1000, 1000)\n",
        "        new_population = max(100, new_population)  # 인구수는 최소 100 이상\n",
        "\n",
        "        self.state = [new_min_wage, weather, new_population]\n",
        "        done = True\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "def build_model(state_shape, action_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer(input_shape=state_shape),\n",
        "        layers.Dense(48, activation='relu'),\n",
        "        layers.Dense(48, activation='relu'),\n",
        "        layers.Dense(action_shape, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "class PolicyGradientAgent:\n",
        "    def __init__(self, state_shape, action_shape, learning_rate=0.0005):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_shape = action_shape\n",
        "        self.model = build_model(state_shape, action_shape)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        self.action_space = np.arange(self.action_shape)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = np.reshape(state, [1, self.state_shape[0]])\n",
        "        probabilities = self.model(state).numpy().flatten()\n",
        "        probabilities = np.nan_to_num(probabilities, nan=1e-10)\n",
        "        probabilities = probabilities / np.sum(probabilities)\n",
        "        action = np.random.choice(self.action_space, p=probabilities)\n",
        "        return action.item()\n",
        "\n",
        "    def train(self, states, actions, rewards):\n",
        "        with tf.GradientTape() as tape:\n",
        "            action_probabilities = self.model(np.array(states))\n",
        "            indices = np.array(actions)\n",
        "            action_masks = tf.one_hot(indices, self.action_shape)\n",
        "            log_probs = tf.reduce_sum(action_masks * tf.math.log(action_probabilities + 1e-10), axis=1)\n",
        "            loss = -tf.reduce_mean(log_probs * rewards)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "env = PricingEnv()\n",
        "state_shape = (3,)\n",
        "action_shape = len(env.action_space)\n",
        "num_models = 10\n",
        "episodes_per_model = 100\n",
        "gamma = 0.99\n",
        "performance_metrics = []\n",
        "price_predictions = []\n",
        "\n",
        "def predict_price(agent, min_wage, weather, population):\n",
        "    state = [min_wage, weather, population]\n",
        "    action = agent.choose_action(state)\n",
        "    predicted_price = env.action_space[action]\n",
        "    return predicted_price\n",
        "\n",
        "for model_index in range(num_models):\n",
        "    agent = PolicyGradientAgent(state_shape, action_shape)\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(episodes_per_model):\n",
        "        state = env.reset()\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(env.action_space[action])\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "\n",
        "        discounted_rewards = []\n",
        "        cumulative_reward = 0\n",
        "        for reward in reversed(rewards):\n",
        "            cumulative_reward = reward + gamma * cumulative_reward\n",
        "            discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "        discounted_rewards = np.array(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-10)\n",
        "\n",
        "        agent.train(states, actions, discounted_rewards)\n",
        "        total_rewards.append(np.sum(rewards))\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    performance_metrics.append(avg_reward)\n",
        "    print(f\"Model {model_index + 1}: Reward = {np.sum(total_rewards)}, Average Reward = {avg_reward}\")\n",
        "\n",
        "    # 가격 예측\n",
        "    min_wage = 12000\n",
        "    weather = 0\n",
        "    population = 500000\n",
        "    predicted_price = predict_price(agent, min_wage, weather, population)\n",
        "    price_predictions.append(predicted_price)\n",
        "    print(f\"Predicted price for products after {episodes_per_model * (model_index + 1)} episodes: {predicted_price}원\")\n",
        "\n",
        "# 평가 지표 출력\n",
        "print(\"Performance metrics per 100 episodes:\")\n",
        "for i, metric in enumerate(performance_metrics):\n",
        "    print(f\"100 episodes from {i*100 + 1} to {(i+1)*100}: Average Reward = {metric}\")\n",
        "\n",
        "# 가격 예측 출력\n",
        "print(\"Predicted prices per 100 episodes:\")\n",
        "for i, price in enumerate(price_predictions):\n",
        "    print(f\"After {i*100 + 100} episodes: Predicted price = {price}원\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "moHrTPioY2Ay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}